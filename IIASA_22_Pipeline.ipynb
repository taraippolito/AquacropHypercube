{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f75f92",
   "metadata": {},
   "source": [
    "# Pipeline building Meta-Model of EPIC Response Curves\n",
    "\n",
    " Step 0: Choose data subset, investigate feature importance for the given subset\n",
    " \n",
    " Step 1: Identify importance of variables \n",
    " \n",
    " Look at a variety of models and methods to determine the best feature importance \n",
    " \n",
    " Step 2: Build Response Curve \n",
    "  \n",
    " Use matching to build response curves \n",
    " \n",
    "  Exact match based on EPIC experiments where they exist (same simulation ID x year, different treatments), propensity score match when set experiments do not exist match (different simulation ID x years, same x variables, different treatments)\n",
    "  \n",
    " Choose the form of the response curve - spheroid \n",
    " \n",
    " Step 3: Build Meta-Model\n",
    " \n",
    " build a machine learning model which learns the relationship between inputs and response curve shape - ensure there are parameters which can be tuned\n",
    " \n",
    " Step 4: Test if tuned meta-model can predict yields  \n",
    " \n",
    " test on czech data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8861ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import statements \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from functools import reduce\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import os\n",
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "from IIASA_22_fxns import get_N_exp, get_season_info, split_data, get_gs_climate, tt_split_scale, random_forest, yield_run_data\n",
    "import tqdm\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84148e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## datasets that may be used \n",
    "# simulation location data - X, Y, SimUID\n",
    "loc = pd.read_csv(\"//Users//taraippolito//Desktop//Desktop_Tara’s_MacBook_Pro//EPIC_local//SimUID_Pts_210_240.txt\", sep = \",\")\n",
    "\n",
    "# simUID site data\n",
    "site_df = pd.read_csv(\"//Users//taraippolito//Desktop//Desktop_Tara’s_MacBook_Pro//EPIC_local//_SimUData//SimUID_List.txt\", sep = \";\")\n",
    "\n",
    "# simulation units, all climate, site data - YEAR IS SEPARATE\n",
    "obs_df = pd.read_csv(\"//Users//taraippolito//Desktop//Desktop_Tara’s_MacBook_Pro//EPIC_local//_SimUData//SimUID_static+clim.csv\")\n",
    "\n",
    "# # clusters\n",
    "# clust = pd.read_csv(\"//Users//taraippolito//Desktop//Desktop_Tara’s_MacBook_Pro//EPIC_local//_SimUData//climate_PCA_x_simUID.csv\")\n",
    "\n",
    "# # Growing season climate variables \n",
    "# GS_clim = pd.read_csv(\"//Users//taraippolito//Desktop//Desktop_Tara’s_MacBook_Pro//EPIC_local//CORN//C_BAU_R00_GSclim.csv\") \n",
    "\n",
    "# climate files \n",
    "# Import simulation unit data and all weather data \n",
    "pet_df = pd.read_csv(\"//Users//taraippolito//Desktop//Desktop_Tara’s_MacBook_Pro//EPIC_local//_Weather//CORN_dyn_rf_BAU_R00_PET.txt\", sep = \",\")\n",
    "prcp_df = pd.read_csv(\"//Users//taraippolito//Desktop//Desktop_Tara’s_MacBook_Pro//EPIC_local//_Weather//CORN_dyn_rf_BAU_R00_PRCP.txt\", sep = \",\")\n",
    "rad_df = pd.read_csv(\"//Users//taraippolito//Desktop//Desktop_Tara’s_MacBook_Pro//EPIC_local//_Weather//CORN_dyn_rf_BAU_R00_RAD.txt\", sep = \",\")\n",
    "tmin_df = pd.read_csv(\"//Users//taraippolito//Desktop//Desktop_Tara’s_MacBook_Pro//EPIC_local//_Weather//CORN_dyn_rf_BAU_R00_TMN.txt\", sep = \",\")\n",
    "tmax_df = pd.read_csv(\"//Users//taraippolito//Desktop//Desktop_Tara’s_MacBook_Pro//EPIC_local//_Weather//CORN_dyn_rf_BAU_R00_TMX.txt\", sep = \",\")\n",
    "vpd_df = pd.read_csv(\"//Users//taraippolito//Desktop//Desktop_Tara’s_MacBook_Pro//EPIC_local//_Weather//CORN_dyn_rf_BAU_R00_VPD.txt\", sep = \",\")\n",
    "cmd_df = pd.read_csv(\"//Users//taraippolito//Desktop//Desktop_Tara’s_MacBook_Pro//EPIC_local//_Weather//CORN_dyn_rf_BAU_R00_CMD.txt\", sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4293c",
   "metadata": {},
   "source": [
    "## Step 0: Choose data subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c5f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### identify data subset to start with \n",
    "C = [\"CORN\"] \n",
    "N = [\"BAU\", \"N01\", \"N50\", \"N100\", \"N250\"]\n",
    "# N = [\"BAU\"]\n",
    "R = [\"R00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17955dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get season-specific info: start of season, end of season, length of season \n",
    "# pull arguments to feed into function in parallel\n",
    "season_info_args = []\n",
    "for c in C:\n",
    "    for n in N: \n",
    "        for r in R: \n",
    "            season_info_args.append((c,n,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73c377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get epic yield data for each treatment \n",
    "yields = []\n",
    "\n",
    "for arg in season_info_args: \n",
    "    yields.append(yield_run_data(arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54d312f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "season info, done.\n",
      "season info, done.\n",
      "season info, done.\n",
      "season info, done.\n",
      "season info, done.\n"
     ]
    }
   ],
   "source": [
    "#### run in parallel \n",
    "# arguments = season_info_args\n",
    "# # PARALLEL \n",
    "# if __name__ == '__main__':\n",
    "#     print (\"in main.\")\n",
    "#     with Pool(4) as pool:\n",
    "#         season_info_dfs = list(tqdm.tqdm(pool.imap(get_season_info, arguments), total=len(arguments)))\n",
    "#         pool.close() \n",
    "\n",
    "#### or run simply if data subset is not too big - e.g. just one crop x treatment \n",
    "season_info_dfs = []\n",
    "for arg in season_info_args: \n",
    "    season_info_dfs.append(get_season_info(arg))\n",
    "    print (\"season info, done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a40a2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Split seasonal info dataframes into smaller dataframes where year, season length, and season start are unique\n",
    "# run in parallel \n",
    "# arguments = season_info_dfs\n",
    "# # PARALLEL\n",
    "# if __name__ == '__main__':\n",
    "#     print (\"in main.\")\n",
    "#     with Pool(4) as pool:\n",
    "#         split_dfs = list(tqdm.tqdm(pool.imap(split_data, arguments), total=len(arguments)))\n",
    "#         pool.close() \n",
    "\n",
    "#### or run simply if data subset is not too big\n",
    "#### output is a list of lists, first dimension is treatment season info df, second dimension is season start/len split\n",
    "split_dfs = []\n",
    "for df in season_info_dfs:\n",
    "    # add lists to split_df list that come out of split_data\n",
    "    split_dfs.append(split_data(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74732261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "treatment done.\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "treatment done.\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "treatment done.\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "treatment done.\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "clim done\n",
      "treatment done.\n"
     ]
    }
   ],
   "source": [
    "#### For the split up seasonal info dataframes, calculate growing season variables for a given climate variable \n",
    "#### NOT IN PARALLEL AS PARALLEL TAKES LONGER \n",
    "climate_dfs = [pet_df, prcp_df, rad_df, tmin_df, tmax_df, vpd_df, cmd_df]\n",
    "GS_dfs_all_szn = []\n",
    "\n",
    "# save growing season climate variables for each treatment \n",
    "ALLGSclimxtreat = []\n",
    "# loop over the treatments\n",
    "for i in range(len(split_dfs)):\n",
    "    # save growing season climate for each climate variable\n",
    "    GSclim_dfs = []\n",
    "    # for each climate variable\n",
    "    for clim_df in climate_dfs: \n",
    "        clim_out_list = []\n",
    "        # go over all the season len splits in a given treatment and get the climate cols for the seasonal split \n",
    "        for df in split_dfs[i]:\n",
    "            # get season x treatment specific engineered variables\n",
    "            clim_out_list.append(get_gs_climate((df, clim_df)))\n",
    "        # for all the splits, concat them vertically then add to the \n",
    "        GSclim_dfs.append(pd.concat(clim_out_list))\n",
    "        print ('clim done')\n",
    "    # drop duplicate variables from new dataframes \n",
    "    concat_clim = pd.concat(GSclim_dfs, axis = 1)\n",
    "    ready = concat_clim.loc[:,~concat_clim.columns.duplicated()]\n",
    "    \n",
    "    ALLGSclimxtreat.append(ready)\n",
    "    print (\"treatment done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd93c5d",
   "metadata": {},
   "source": [
    "## Step 1: Identify Importance of Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec8b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for testing purposes \n",
    "ALLGSclimxtreat = [GS_clim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c22fb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished TT split.\n",
      "finished fitting scaler.\n",
      "finished X transformation.\n",
      "finished y transformation.\n"
     ]
    }
   ],
   "source": [
    "#### USE A RANDOM FOREST REGRESSION MODEL TO IDENTIFY FEATURE IMPORTANCE \n",
    "#### Build random forest with all features that data you would like to predict has available - \n",
    "#### we only want feature importance for features we can use in predictions \n",
    "\n",
    "# get train test sets for each treatment \n",
    "train_test = []\n",
    "all_data_sets = []\n",
    "\n",
    "# for each treatment, get a predictor set and a target set \n",
    "for i in range(len(ALLGSclimxtreat)): \n",
    "    # pull site specific data \n",
    "    # One-hot encode the soil hydrological group \n",
    "    HSG_dummy = pd.get_dummies(obs_df.HSG2, prefix = \"HSG\")\n",
    "    dummy_add = pd.concat([obs_df, HSG_dummy], axis = 1)\n",
    "    \n",
    "    # MERGE THE SEPARATE DATA PIECES TOGETHER \n",
    "    predictor_data = pd.merge(dummy_add, ALLGSclimxtreat[i], how = \"left\", on = [\"SimUID\", \"YR\"]).drop(['Unnamed: 0_x', 'Unnamed: 0_y', 'SimU', 'NUTS2',\n",
    "       'LCF3', 'HRU', 'ELEV_CAT', 'SLP_CAT', 'TEXTURE', 'DTR', 'STONES',\n",
    "       'ELEV', 'HSG2'], axis = 1)\n",
    "    predictor_vars = list(predictor_data.columns)[2:]\n",
    "    target_data = yields[i][['SimUID', 'YR', 'SCEN', 'YLDG', 'BIOM']]\n",
    "    all_data = pd.merge(predictor_data, target_data, how = \"left\", on = [\"SimUID\", 'YR'])\n",
    "    # save the big datasets\n",
    "    all_data_sets.append(all_data)\n",
    "    \n",
    "    # get train test splits\n",
    "    X_train, X_test, y_train, y_test = tt_split_scale(all_data, \"YLDG\")\n",
    "    # append to list \n",
    "    train_test.append([X_train, X_test, y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47a347f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importance for each treatment and scores\n",
    "feat_imp_list = [] \n",
    "scores = []\n",
    "\n",
    "for tts in train_test: \n",
    "    y_predicted, score, feat_imp = random_forest(tts[0], tts[1], tts[2], tts[3], 50, 20)\n",
    "    feat_df = pd.DataFrame(feat_imp, index = predictor_vars, columns = ['feat_imp']).sort_values('feat_imp', ascending = False)\n",
    "    \n",
    "    feat_imp_list.append(feat_df)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea19cfde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RADavGS</th>\n",
       "      <td>0.008603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JUN_PRCP</th>\n",
       "      <td>0.007917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGG_PET</th>\n",
       "      <td>0.007594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGG_PRCP</th>\n",
       "      <td>0.007409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KS_SUB2</th>\n",
       "      <td>0.007244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FEB_RAD</th>\n",
       "      <td>0.007092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TMNskGS</th>\n",
       "      <td>0.007068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN_RAD</th>\n",
       "      <td>0.006850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JUN_TMN</th>\n",
       "      <td>0.006708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN_PRCP</th>\n",
       "      <td>0.006481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOV_PET</th>\n",
       "      <td>0.006403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TMXskGS</th>\n",
       "      <td>0.006210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUG_TMN</th>\n",
       "      <td>0.006198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAY_TMX</th>\n",
       "      <td>0.006073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGG_RAD</th>\n",
       "      <td>0.006055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RADsumGS</th>\n",
       "      <td>0.006009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUG_VPD</th>\n",
       "      <td>0.005785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CEC_SUB</th>\n",
       "      <td>0.005347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OC_TOP</th>\n",
       "      <td>0.005156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KS_TOP2</th>\n",
       "      <td>0.005146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feat_imp\n",
       "RADavGS    0.008603\n",
       "JUN_PRCP   0.007917\n",
       "AGG_PET    0.007594\n",
       "AGG_PRCP   0.007409\n",
       "KS_SUB2    0.007244\n",
       "FEB_RAD    0.007092\n",
       "TMNskGS    0.007068\n",
       "MEAN_RAD   0.006850\n",
       "JUN_TMN    0.006708\n",
       "MEAN_PRCP  0.006481\n",
       "NOV_PET    0.006403\n",
       "TMXskGS    0.006210\n",
       "AUG_TMN    0.006198\n",
       "MAY_TMX    0.006073\n",
       "AGG_RAD    0.006055\n",
       "RADsumGS   0.006009\n",
       "AUG_VPD    0.005785\n",
       "CEC_SUB    0.005347\n",
       "OC_TOP     0.005156\n",
       "KS_TOP2    0.005146"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp_list[0].iloc[20:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a955e",
   "metadata": {},
   "source": [
    "## Step 2: Build Response Curves of Identified Important variables\n",
    "1. To build response curves with baked in EPIC experiments, use simulation unit and year to look at responses (e.g. N fertilized at SimUID 3 in 1985, with N rates 0, 50, 100, 250, BAU)\n",
    "2. To build response curves for experiments outside EPIC defined experiments, use matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build matched data subsets for each response curve being built \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e4ab3",
   "metadata": {},
   "source": [
    "## Step 3: Build Meta-Model \n",
    "1. Meta-model is an ensemble of polynomial regression functions (response curves) \n",
    "2. Ensemble base models are weighted according to feature importance derived from random forest \n",
    "3. Final output is weighted average from predictions from base models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370673ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
